#!/usr/bin/env python
import time

import os.path as osp
from pepper_2d_simulator import PepperRLEnv, populate_PepperRLEnv_args, check_PepperRLEnv_args
from map2d import Map2D
import numpy as np
from gym.spaces.box import Box

from trainPPO import parse_training_args

rew_hist_filepath = "/tmp/instant_rewards.txt"



def parse_args():
    import argparse
    ## Arguments
    parser = argparse.ArgumentParser(description='Test node for the pepper RL path planning algorithm.')
#     parser.add_argument('--map-folder',
#             type=str,
#             default=".",
#             )
#     parser.add_argument('--map-name',
#             type=str,
#             default="empty",
#             )
    parser.add_argument(
            '--linear-controller',
            action='store_true',
            help='Uses a linear controller instead of the trained policy',
    )
    parser.add_argument(
            '--write-rewards',
            dest='write_rewards',
            action='store_true',
            help='Write rewards at every timestep to file',
    )
    parser.add_argument(
            '--no-reset',
            action='store_true',
            help='keep the episode running even if it should be reset',
    )
    # add PPO args
    ARGS, unknown = parse_training_args(
        ignore_unknown=True,
        parser=parser,
        env_populate_args_func=populate_PepperRLEnv_args
    )

    # deal with unknown arguments
    # ROS appends some weird args, ignore those, but not the rest
    if unknown:
        rosparser = argparse.ArgumentParser()
        rosparser.add_argument(
                '__log:')
        rosparser.add_argument(
                '__name:')
        rosargs = rosparser.parse_args(unknown)

    return ARGS

if __name__ == '__main__':
    args = parse_args()

    # Source the current dir to be able to import map2d.py and others
    import os, sys
    scriptdir = os.path.dirname(os.path.realpath(__file__))
    print("Script directory: {}".format(scriptdir))
    sys.path.insert(0,scriptdir)
    # Load the map
    map_folder = args.map_folder
    map_name = args.map_name
    from CMap2D import CMap2D
    map_ = CMap2D(map_folder, map_name)
    print("Map '{}' loaded.".format(map_name))
    # test
    ROS = not args.no_ros


    if args.write_rewards:
        print("Writing rewards to {}".format(rew_hist_filepath))
        with open(rew_hist_filepath, 'w') as f:
            f.write('')


    # ------- PPO1 DEPRECATED ------------------------
    # Run policy
    if False:
        env = PepperRLEnv(args)
        # run linear controller
        if args.linear_controller:
            print("Running linear controller for all agents.")
            for i in range(10000):
                ac = env.get_linear_controller_action()
                ob, rew, new, _ = env.step(ac)
                if not args.no_reset:
                    env.reset(new)
                if args.write_rewards:
                    with open(rew_hist_filepath, 'ab') as f:
                        np.savetxt(f, rew[None], delimiter=',')
        else:
            # RL multi-agent simulator
            import cnn_policy
            import baselines.common.tf_util as U
            sess = U.single_threaded_session()
            sess.__enter__()
            def policy_fn(name, ob_space, ac_space): #pylint: disable=W0613
                return cnn_policy.CnnPolicy(name=name, ob_space=ob_space, ac_space=ac_space)
            ob_space = env.observation_space
            ac_space = env.action_space
            pi = policy_fn("pi", ob_space, ac_space) # Construct network for new policy
            if not args.no_reset:
                ob = env.reset(new)
            stochastic = False
            pi.load_variables("/tmp/rlnav_model")
            ac, vpred = pi.act(stochastic, ob)
            for i in range(10000):
                ac, vpred = pi.act(stochastic, ob)
                ob, rew, new, _ = env.step(ac)
                if args.write_rewards:
                    with open(rew_hist_filepath, 'ab') as f:
                        np.savetxt(f, rew[None], delimiter=',')
    # ------------------------------------------------

    from PPO import MlpPPO
    from smallMlp import smallMlp
    import trainPPO
    # args
    PPOargs, _ = parse_training_args(
        env_populate_args_func=populate_PepperRLEnv_args
    )
    PPOargs.no_ros = args.no_ros
    check_PepperRLEnv_args(PPOargs)


    env = PepperRLEnv(args=PPOargs)

    # run linear controller
    if args.linear_controller:
        import cProfile
        print("Running linear controller for all agents.")
        for i in range(10000):
            ac = env.get_linear_controller_action()
#             print(cProfile.run("ob, rew, new, _ = env.step(ac)"))
            ob, rew, new, _ = env.step(ac)
            if not args.no_reset:
                if np.any(new):
                    env.reset(new)
            if args.write_rewards:
                with open(rew_hist_filepath, 'ab') as f:
                    np.savetxt(f, rew[None], delimiter=',')
    # run policy
    else:
        if PPOargs.resume_from == '':
            raise ValueError("No model provided. please provide a checkpoint to load using either --resume-latest or --resume-from")

        if PPOargs.policy == "MlpPolicy":
            policy_type = MlpPPO
        elif PPOargs.policy == "SmallMlpPolicy":
            policy_type = smallMlp
        else:
            raise NotImplementedError
        ppo = policy_type(env.action_space, env.observation_space,'testPPO', PPOargs)
        ob = env.reset()
        import tensorflow as tf
        with tf.Session() as sess:
            # uncomment for compatibility with older models
    #         saver = tf.train.Saver(max_to_keep=5, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))
            saver = tf.train.Saver()
            sess.run(tf.global_variables_initializer())
            ppo.load_best_model(sess, saver)
            try:
                for i in range(10000):
                    ac, _ = ppo.choose_action(ob, sess)
                    ob, rew, new, _ = env.step(ac)
                    if not args.no_reset:
                        if np.any(new):
                            ob = env.reset(new)
                    if args.write_rewards:
                        with open(rew_hist_filepath, 'ab') as f:
                            np.savetxt(f, ac[0,:][None], delimiter=',')
                    if i > 0 and i % PPOargs.max_episode_length == 0:
                        print("Max episode length reached.")
            except KeyboardInterrupt:
                env._ros_shutdown("SIGINT")

